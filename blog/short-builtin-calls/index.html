<!doctype html><html lang=zh-CN><meta charset=utf-8><title>Short builtin calls · V8</title><meta content="width=device-width,initial-scale=1" name=viewport><meta content="dark light" name=color-scheme><link href=/_css/main.css rel=stylesheet><link href=/.webmanifest rel=manifest><meta content=#4285F4 name=theme-color><link href=/blog.atom rel=alternate title="V8 Atom feed" type=application/atom+xml><link href=/features.atom rel=alternate title="V8 JS/Wasm features Atom feed" type=application/atom+xml><script>document.documentElement.className+=' js'</script><meta content="In V8 v9.1 we’ve temporarily unembedded the builtins on desktop to avoid performance issues resulting from far indirect calls." name=description><header id=header><h1><a href=/ >V8</a></h1><a href=#navigation-toggle id=nav-toggle>显示导肮</a><nav><ul><li><a href=/ >主页</a><li class=active><a href=/blog/ >博客</a><li><a href=/docs/ >文档</a><li><a href=/features title="JavaScript 和 WebAssembly 的新特性">JS/Wasm 新特性</a><li><a href=/grant>Research</a></ul></nav></header><main id=main><article itemscope itemtype=http://schema.org/BlogPosting><header><h1 itemprop=headline>Short builtin calls</h1><p class=meta>发布时间 <time datetime="2021-05-06 00:00:00" itemprop=datePublished title="2021-05-06 00:00:00">2021-05-06</time> · 标签： <a href=/blog/tags/javascript/ class=tag>JavaScript</a></header><div itemprop=articleBody><p>In V8 v9.1 we’ve temporarily disabled <a href=https://v8.dev/blog/embedded-builtins>embedded builtins</a> on desktop. While embedding builtins significantly improves memory usage, we’ve realized that function calls between embedded builtins and JIT compiled code can come at a considerable performance penalty. This cost depends on the microarchitecture of the CPU. In this post we’ll explain why this is happening, what the performance looks like, and what we’re planning to do to resolve this long-term.<h2 id=code-allocation>Code allocation <a href=#code-allocation class=bookmark>#</a></h2><p>Machine code generated by V8’s just-in-time (JIT) compilers is allocated dynamically on memory pages owned by the VM. V8 allocates memory pages within a contiguous address space region, which itself either lies somewhere randomly in memory (for <a href=https://en.wikipedia.org/wiki/Address_space_layout_randomization>address space layout randomization</a> reasons), or somewhere inside of the 4-GiB virtual memory cage we allocate for <a href=https://v8.dev/blog/pointer-compression>pointer compression</a>.<p>V8 JIT code very commonly calls into builtins. Builtins are essentially snippets of machine code that are shipped as part of the VM. There are builtins that implement full JavaScript standard library functions, such as <a href=https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_objects/Function/bind><code>Function.prototype.bind</code></a>, but many builtins are helper snippets of machine code that fill in the gap between the higher-level semantics of JS and the low-level capabilities of the CPU. For example, if a JavaScript function wants to call another JavaScript function, it is common for the function implementation to call a <code>CallFunction</code> builtin that figures out how the target JavaScript function should be called; i.e., whether it’s a proxy or a regular function, how many arguments it expects, etc. Since these snippets are known when we build the VM, they are "embedded" in the Chrome binary, which means that they end up within the Chrome binary code region.<h2 id=direct-vs.-indirect-calls>Direct vs. indirect calls <a href=#direct-vs.-indirect-calls class=bookmark>#</a></h2><p>On 64-bit architectures, the Chrome binary, which includes these builtins, lies arbitrarily far away from JIT code. With the <a href=https://en.wikipedia.org/wiki/X86-64>x86-64</a> instruction set, this means we can’t use direct calls: they take a 32-bit signed immediate that’s used as an offset to the address of the call, and the target may be more than 2 GiB away. Instead, we need to rely on indirect calls through a register or memory operand. Such calls rely more heavily on prediction since it’s not immediately apparent from the call instruction itself what the target of the call is. On <a href=https://en.wikipedia.org/wiki/AArch64>ARM64</a> we can’t use direct calls at all since the range is limited to 128 MiB. This means that in both cases we rely on the accuracy of the CPU's indirect branch predictor.<h2 id=indirect-branch-prediction-limitations>Indirect branch prediction limitations <a href=#indirect-branch-prediction-limitations class=bookmark>#</a></h2><p>When targeting x86-64 it would be nice to rely on direct calls. It should reduce strain on the indirect branch predictor as the target is known after the instruction is decoded, but it also doesn't require the target to be loaded into a register from a constant or memory. But it's not just the obvious differences visible in the machine code.<p>Due to <a href=https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html>Spectre v2</a> various device/OS combinations have turned off indirect branch prediction. This means that on such configurations we’ll get very costly stalls on function calls from JIT code that rely on the <code>CallFunction</code> builtin.<p>More importantly, even though 64-bit instruction set architectures (the “high-level language of the CPU”) support indirect calls to far addresses, the microarchitecture is free to implement optimisations with arbitrary limitations. It appears common for indirect branch predictors to presume that call distances do not exceed a certain distance (e.g., 4GiB), requiring less memory per prediction. E.g., the <a href=https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf>Intel Optimization Manual</a> explicitly states:<blockquote><p>For 64-bit applications, branch prediction performance can be negatively impacted when the target of a branch is more than 4 GB away from the branch.</blockquote><p>While on ARM64 the architectural call range for direct calls is limited to 128 MiB, it turns out that <a href=https://en.wikipedia.org/wiki/Apple_M1>Apple’s M1</a> chip has the same microarchitectural 4 GiB range limitation for indirect call prediction. Indirect calls to a call target further away than 4 GiB always seem to be mispredicted. Due to the particularly large <a href=https://en.wikipedia.org/wiki/Re-order_buffer>re-order buffer</a> of the M1, the component of the CPU that enables future predicted instructions to be executed speculatively out-of-order, frequent misprediction results in an exceptionally large performance penalty.<h2 id=temporary-solution%3A-copy-the-builtins>Temporary solution: copy the builtins <a href=#temporary-solution%3A-copy-the-builtins class=bookmark>#</a></h2><p>To avoid the cost of frequent mispredictions, and to avoid unnecessarily relying on branch prediction where possible on x86-64, we’ve decided to temporarily copy the builtins into V8's pointer compression cage on desktop machines with enough memory. This puts the copied builtin code close to dynamically generated code. The performance results heavily depend on the device configuration, but here are some results from our performance bots:<figure><img alt="" height=371 loading=lazy src=/_img/short-builtin-calls/v8-browsing.svg width=600><figcaption>Browsing benchmarks recorded from live pages</figcaption></figure><figure><img alt="" height=371 loading=lazy src=/_img/short-builtin-calls/benchmarks.svg width=600><figcaption>Benchmark score improvement</figcaption></figure><p>Unembedding builtins does increase memory usage on affected devices by 1.2 to 1.4 MiB per V8 instance. As a better long-term solution we’re looking into allocating JIT code closer to the Chrome binary. That way we can re-embed the builtins to regain the memory benefits, while additionally improving the performance of calls from V8-generated code to C++ code.</div><footer><div><picture><source srcset="/_img/avatars/toon-verwaest.avif, /_img/avatars/toon-verwaest@2x.avif 2x" type=image/avif><img alt="" height=96 loading=lazy src=/_img/avatars/toon-verwaest.jpg width=96 srcset="/_img/avatars/toon-verwaest@2x.jpg 2x"></picture><p>作者：<a href=https://twitter.com/tverwaes>Toon Verwaest</a>, The Big Short.</div><a href=https://twitter.com/v8js/status/1394267917013897216 class=retweet>Retweet this article!</a></footer></article></main><footer id=footer><div><nav><a href=https://v8.dev/blog/short-builtin-calls>原文</a> · <a href=/logo/ >商标</a> · <a href=/terms/ >条款</a> · <a href=https://policies.google.com/privacy/ >隐私</a> · <a href=https://twitter.com/v8js rel="me nofollow">Twitter</a> · <a href=https://github.com/justjavac/v8.js.cn/tree/master/./src/blog/short-builtin-calls.md rel=nofollow>在 GitHub 编辑此页面</a></nav><dark-mode-toggle dark="Light Theme" light="Dark Theme" permanent></dark-mode-toggle></div><p><small>如无特殊说明，此 V8 项目中使用到的所有示例代码均基于 <a href=https://chromium.googlesource.com/v8/v8.git/+/master/LICENSE>V8's BSD-style license</a> 发布。页面中的文字内容采用 <a href=https://creativecommons.org/licenses/by/3.0/ >the Creative Commons Attribution 3.0 License</a> 进行许可。更详细的信息可以在 <a href=/terms#site-policies>站点策略</a> 中找到。</small></footer><script src=/_js/dark-mode-toggle.mjs type=module></script><script src=/_js/main.mjs type=module></script><script src=/_js/legacy.js nomodule></script>